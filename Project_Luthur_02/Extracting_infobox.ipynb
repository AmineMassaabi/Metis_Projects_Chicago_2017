{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool #witness the power\n",
    "import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from datetime import datetime\n",
    "from helper_functions import *\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code that was used to extract every movie's infobox on wikipedia that I had in my movie-dictionary.\n",
    "\n",
    "This was an extrmemly iterative process, however, it was worth the time it took to create as it successfully obtained over 95% of the information I required. This notebook was **critical** in the success of this project.\n",
    "\n",
    "I hope the code below may serve as some inspiration to others when scraping wikipedia infoboxes - it is no easy task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the wikipedia objects for all of our associated movies. Also, for those movies which had no wikipedia object, their information was entered manually (yes, manually - i know you're sighing. I was sighing too).\n",
    "\n",
    "This notebook took approximately 6 hours to design. It was an iterative process. That is to say, many things broke in the extract_wiki_infobox function at the start.\n",
    "\n",
    "As such, in order to save time and ensure some semblance of data integrity. I had to removed around 10-15 movies from the movies database.\n",
    "\n",
    "In the end, we have 900 movies in our database. That does NOT mean however, that all of the fields for each movies have valid values. Wikipedia and rotton tomatoes do not hold all the answers with regards to budget, box office amount etc. As such, there will be many movies that have a range of NaN values.\n",
    "\n",
    "For simplicity we will impute these - yes, this is NOT best practice, as movies span several years and the effects of inflation are not accounted for.\n",
    "\n",
    "Bear in mind, I was given a week and half to put this together!\n",
    "\n",
    "The extract_wiki_infobox does its best to ensure some standard form for all fields. I do this to minimize the cleaning required once I get the movie database dictionary into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_db = unpickle_object(\"movie_database.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(movie_db.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_wiki_infobox():\n",
    "    \n",
    "    regex = r\" *\\[[^\\]]*.*\"\n",
    "    regex2 = r\" *\\([^\\)].*\"\n",
    "    regex3 = r\" *\\/[^\\)]*.*\"\n",
    "    regex4 = r\" *\\,[^\\)].*\"\n",
    "    regex5 = r\".*(?=\\$)\"\n",
    "    regex6 = r\".*(?=\\£)\"\n",
    "    regex7 = r\"\\–.*$\"\n",
    "    regex_date = r\"^[^\\(]*\"\n",
    "    regex_date_2 = r\" *\\)[^\\)].*\"\n",
    "    subset=''\n",
    "\n",
    "\n",
    "\n",
    "    for key in sorted(movie_db.keys()):\n",
    "        if len(movie_db[key]) == 6:\n",
    "            html_url = movie_db[key][-1].url\n",
    "            info_box_dictionary = {}\n",
    "            soup = BeautifulSoup(movie_db[key][5].html(), 'lxml')\n",
    "            wikipedia_api_info = soup.find(\"table\",{\"class\":\"infobox vevent\"})\n",
    "\n",
    "            info_box_dictionary = {}\n",
    "\n",
    "            for tr in wikipedia_api_info.find_all('tr'):\n",
    "                if tr.find('th'):\n",
    "                    info_box_dictionary[tr.find('th').text] = tr.find('td')\n",
    "\n",
    "            try: #done\n",
    "                date = info_box_dictionary['Release date'].text\n",
    "                date = re.sub(regex_date, subset, date)\n",
    "                try:\n",
    "                    date = date.split()[0].strip(\"(\").strip(\")\")\n",
    "                    date = re.sub(regex_date_2,subset, date)\n",
    "                except IndexError:\n",
    "                    date = info_box_dictionary['Release date'].text\n",
    "                    date = re.sub(regex_date, subset, date)\n",
    "            except KeyError:\n",
    "                date = np.nan\n",
    "\n",
    "            try: #done\n",
    "                runtime = info_box_dictionary['Running time'].text\n",
    "                runtime = re.sub(regex, subset, runtime)\n",
    "                runtime = re.sub(regex2, subset, runtime)\n",
    "            except KeyError:\n",
    "                runtime = np.nan\n",
    "\n",
    "            try: #done\n",
    "                boxoffice = info_box_dictionary['Box office'].text\n",
    "                boxoffice = re.sub(regex, subset, boxoffice)\n",
    "                boxoffice = re.sub(regex6, subset, boxoffice)\n",
    "                boxoffice = re.sub(regex5, subset, boxoffice)\n",
    "                if \"billion\" not in boxoffice:\n",
    "                    boxoffice = re.sub(regex7, subset, boxoffice)\n",
    "                    boxoffice = re.sub(regex2, subset, boxoffice)\n",
    "            except KeyError:\n",
    "                boxoffice = np.nan\n",
    "\n",
    "            try:#done\n",
    "                budget = info_box_dictionary['Budget'].text\n",
    "                budget = re.sub(regex, subset, budget)\n",
    "                budget = re.sub(regex7, subset, budget)\n",
    "                if \"$\" in budget:\n",
    "                    budget = re.sub(regex5, subset, budget)\n",
    "                    budget = re.sub(regex2, subset, budget)\n",
    "                if \"£\" in budget:\n",
    "                    budget = re.sub(regex6, subset, budget)\n",
    "                    budget = re.sub(regex2, subset, budget)\n",
    "                budget = re.sub(regex5, subset, budget)\n",
    "            except KeyError:\n",
    "                budget = np.nan\n",
    "\n",
    "            try:#done\n",
    "                country = info_box_dictionary['Country'].text.strip().lower()\n",
    "                country = re.sub(regex, subset, country) #cleans out a lot of gunk\n",
    "                country = re.sub(regex2, subset, country)\n",
    "                country = re.sub(regex3, subset, country)\n",
    "                country = re.sub(regex4, subset, country)\n",
    "                country = country.split()\n",
    "                if country[0] == \"united\" and country[1] == \"states\":\n",
    "                    country = country[0]+\" \"+country[1]\n",
    "                elif country[0] ==\"united\" and country[1] == \"kingdom\":\n",
    "                    country = country[0] +\" \"+ country[1]\n",
    "                else:\n",
    "                    country = country[0]\n",
    "            except KeyError:\n",
    "                country = np.nan\n",
    "\n",
    "            try:#done\n",
    "                language = info_box_dictionary['Language'].text.strip().split()[0]\n",
    "                language = re.sub(regex, subset, language)\n",
    "            except KeyError:\n",
    "                language = np.nan\n",
    "\n",
    "            movie_db[key].append(date)\n",
    "            movie_db[key].append(runtime)\n",
    "            movie_db[key].append(boxoffice)\n",
    "            movie_db[key].append(budget)\n",
    "            movie_db[key].append(country)\n",
    "            movie_db[key].append(language)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_wiki_infobox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
