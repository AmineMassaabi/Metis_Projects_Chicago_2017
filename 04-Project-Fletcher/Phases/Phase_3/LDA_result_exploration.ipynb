{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will explore some of the results obtained by running the `run_lda.py` file!\n",
    "\n",
    "There are 3 main things that I do:\n",
    "\n",
    "- I take the names of those twitter handles which LDA couldnt not find a substantive 'topic(s)' for. That is to say, these twitter hadnles have been inactive for a while, have tweets that are extrmemely scattered or just simply do not have enough tweets for any analysis to be carried out. I place the names of these twitter users in a pickled list called `LDA_identified_bad_handles.pkl`.\n",
    "\n",
    "\n",
    "\n",
    "- I take the names of these bag handles and remove them from the list containing all the handles of the 2nd degree connnections we started with. I do this as I will need a 'pure' list of handles when running our tf-idf analysis. I will only be running tf-idf analysis on those twitter users that have valid LDA results. I place this list of valid hadnles in a pickle file called `verified_handles_lda.pkl`\n",
    "\n",
    "\n",
    "\n",
    "- I delete the the 'bad handle' dictionaries from our list of dictionaries. I pickle the resulting list into a file called `final_database_lda_verified.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst = unpickle_object(\"2nd_degree_connections_LDA_complete.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meereve': {'content': ['great blog thx minor detail model shld nby theta',\n",
       "   'tensorflow nice doc candidate sampler',\n",
       "   'cool good pointer choose nce play non word corpus good',\n",
       "   'port excellent lda2vec',\n",
       "   'neural aartist',\n",
       "   'finally public molecular autoencoder let interpolate gradient base optimization compound',\n",
       "   'gif get',\n",
       "   'introduce variational autoencoder prose code tensorflow python',\n",
       "   'mris close see synesthesia experience machine',\n",
       "   'tale variational autoencoder overzealous globb',\n",
       "   'blog go hood variational autoencoder',\n",
       "   'vaes infuse math tensorflow',\n",
       "   'introduce variational autoencoder prose code',\n",
       "   'introduce variational autoencoder prose code blog',\n",
       "   'note cite post come soon thx',\n",
       "   'aha think line miss note paper thumbs easy fix nice result',\n",
       "   'humbly offer machine learn gif',\n",
       "   'build app visualize transcript',\n",
       "   'find set',\n",
       "   'stripper',\n",
       "   'cell lar device',\n",
       "   'cell lar device',\n",
       "   'feign surprise',\n",
       "   'trippy hil',\n",
       "   'monterrey jelly',\n",
       "   'go monterrey bay aquarium jelly',\n",
       "   'feel great commit lowre branch',\n",
       "   'sher minn like',\n",
       "   'plz patient wait retweet',\n",
       "   'set justwonder',\n",
       "   'want know set write file obvs',\n",
       "   'write file loser',\n",
       "   'stump',\n",
       "   'memoriez',\n",
       "   'remember day',\n",
       "   'canoe abstract',\n",
       "   'set ception',\n",
       "   'look mirror',\n",
       "   'trippy',\n",
       "   'mean tho amirite',\n",
       "   'metaaaa',\n",
       "   'camera ready',\n",
       "   'know base',\n",
       "   'interrobang',\n",
       "   'save tmp file justwonder',\n",
       "   'error',\n",
       "   'look',\n",
       "   'debug',\n",
       "   'set',\n",
       "   'hpy hllwn',\n",
       "   'canoe',\n",
       "   'existentialgarfield',\n",
       "   'nyc',\n",
       "   'glitchy',\n",
       "   'daybreaker',\n",
       "   'try size',\n",
       "   'hey goin',\n",
       "   'like cluster alogrithm kmean',\n",
       "   'set',\n",
       "   'brown cow',\n",
       "   'skip header body datum',\n",
       "   'length uri',\n",
       "   'plz',\n",
       "   'like thegetty',\n",
       "   'murakami',\n",
       "   'try',\n",
       "   'think base64 encode',\n",
       "   'look',\n",
       "   'whataboutnow',\n",
       "   'rainyday nyc',\n",
       "   'try',\n",
       "   'set swear',\n",
       "   'laurapalmer',\n",
       "   'bear coenbrother',\n",
       "   'year',\n",
       "   'like woodcut',\n",
       "   'deadrabbit',\n",
       "   'help',\n",
       "   'book',\n",
       "   'spooky',\n",
       "   'meta inception',\n",
       "   'civilwarhair',\n",
       "   'catbookcat',\n",
       "   'berry',\n",
       "   'abstract',\n",
       "   'like vuillard',\n",
       "   'hey setbot think',\n",
       "   'blackandgold',\n",
       "   'puppy',\n",
       "   'perspective',\n",
       "   'usa usa',\n",
       "   'nice light huh',\n",
       "   'tricky',\n",
       "   'think jmw turner',\n",
       "   'come card game prefer set bridge',\n",
       "   'wassup',\n",
       "   'civilwarhair',\n",
       "   'catbookcat',\n",
       "   'berries',\n",
       "   'abstract',\n",
       "   'like vuillard?',\n",
       "   'hey setbot, think',\n",
       "   'though blackandgold',\n",
       "   'puppies',\n",
       "   \"it's perspective\",\n",
       "   'usa usa',\n",
       "   'nice light, huh',\n",
       "   \"here's tricky one\",\n",
       "   'think jmw turner',\n",
       "   'comes card games, prefer set bridge',\n",
       "   'wassup'],\n",
       "  'favorite_count': [1,\n",
       "   1,\n",
       "   1,\n",
       "   29,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   15,\n",
       "   0,\n",
       "   7,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   10,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'hashtags': [[],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['Tensorflow', 'python'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['strippers'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['justwondering'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['canoe', 'abstract'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['interrobang'],\n",
       "   [],\n",
       "   ['justwondering'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['hpy', 'hllwn'],\n",
       "   ['canoe'],\n",
       "   ['existentialgarfield'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['kmeans'],\n",
       "   ['set'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['thegetty'],\n",
       "   ['murakami'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['whataboutnow'],\n",
       "   ['rainyday', 'nyc'],\n",
       "   [],\n",
       "   [],\n",
       "   ['laurapalmer'],\n",
       "   ['bears', 'coenbrothers'],\n",
       "   [],\n",
       "   [],\n",
       "   ['deadrabbit'],\n",
       "   [],\n",
       "   [],\n",
       "   ['books'],\n",
       "   [],\n",
       "   ['meta', 'inception'],\n",
       "   ['civilwarhair'],\n",
       "   ['catbookcat'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   ['blackandgold'],\n",
       "   ['puppies'],\n",
       "   ['perspective'],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   []],\n",
       "  'retweet_count': [0,\n",
       "   0,\n",
       "   0,\n",
       "   7,\n",
       "   0,\n",
       "   229,\n",
       "   1,\n",
       "   20,\n",
       "   1,\n",
       "   15,\n",
       "   6,\n",
       "   1,\n",
       "   39,\n",
       "   8,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   8,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  'tokenized_tweets': ['great blog thx minor detail model shld nby theta',\n",
       "   'tensorflow nice doc candidate sampler',\n",
       "   'cool good pointer choose nce play non word corpus good',\n",
       "   'port excellent lda2vec',\n",
       "   'neural aartist',\n",
       "   'finally public molecular autoencoder let interpolate gradient base optimization compound',\n",
       "   'gif get',\n",
       "   'introduce variational autoencoder prose code tensorflow python',\n",
       "   'mris close see synesthesia experience machine',\n",
       "   'tale variational autoencoder overzealous globb',\n",
       "   'blog go hood variational autoencoder',\n",
       "   'vaes infuse math tensorflow',\n",
       "   'introduce variational autoencoder prose code',\n",
       "   'introduce variational autoencoder prose code blog',\n",
       "   'note cite post come soon thx',\n",
       "   'aha think line miss note paper thumbs easy fix nice result',\n",
       "   'humbly offer machine learn gif',\n",
       "   'build app visualize transcript',\n",
       "   'find set',\n",
       "   'stripper',\n",
       "   'cell lar device',\n",
       "   'cell lar device',\n",
       "   'feign surprise',\n",
       "   'trippy hil',\n",
       "   'monterrey jelly',\n",
       "   'go monterrey bay aquarium jelly',\n",
       "   'feel great commit lowre branch',\n",
       "   'sher minn like',\n",
       "   'plz patient wait retweet',\n",
       "   'set justwonder',\n",
       "   'want know set write file obvs',\n",
       "   'write file loser',\n",
       "   'stump',\n",
       "   'memoriez',\n",
       "   'remember day',\n",
       "   'canoe abstract',\n",
       "   'set ception',\n",
       "   'look mirror',\n",
       "   'trippy',\n",
       "   'mean tho amirite',\n",
       "   'metaaaa',\n",
       "   'camera ready',\n",
       "   'know base',\n",
       "   'interrobang',\n",
       "   'save tmp file justwonder',\n",
       "   'error',\n",
       "   'look',\n",
       "   'debug',\n",
       "   'set',\n",
       "   'hpy hllwn',\n",
       "   'canoe',\n",
       "   'existentialgarfield',\n",
       "   'nyc',\n",
       "   'glitchy',\n",
       "   'daybreaker',\n",
       "   'try size',\n",
       "   'hey goin',\n",
       "   'like cluster alogrithm kmean',\n",
       "   'set',\n",
       "   'brown cow',\n",
       "   'skip header body datum',\n",
       "   'length uri',\n",
       "   'plz',\n",
       "   'like thegetty',\n",
       "   'murakami',\n",
       "   'try',\n",
       "   'think base64 encode',\n",
       "   'look',\n",
       "   'whataboutnow',\n",
       "   'rainyday nyc',\n",
       "   'try',\n",
       "   'set swear',\n",
       "   'laurapalmer',\n",
       "   'bear coenbrother',\n",
       "   'year',\n",
       "   'like woodcut',\n",
       "   'deadrabbit',\n",
       "   'help',\n",
       "   'book',\n",
       "   'spooky',\n",
       "   'meta inception',\n",
       "   'civilwarhair',\n",
       "   'catbookcat',\n",
       "   'berry',\n",
       "   'abstract',\n",
       "   'like vuillard',\n",
       "   'hey setbot think',\n",
       "   'blackandgold',\n",
       "   'puppy',\n",
       "   'perspective',\n",
       "   'usa usa',\n",
       "   'nice light huh',\n",
       "   'tricky',\n",
       "   'think jmw turner',\n",
       "   'come card game prefer set bridge',\n",
       "   'wassup']}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[8] #an example of a bad handle dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handle_names = []\n",
    "for dictionary in lst:\n",
    "    name = list(dictionary.keys())\n",
    "    handle_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handle_names = sum(handle_names, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emeader\n",
      "nickwooduk\n",
      "raulgarreta\n",
      "stevenkuyan\n"
     ]
    }
   ],
   "source": [
    "#an example of me finding which user's in my LDA results tweet about \"machine\" --> alluding to \"machine learning\"\n",
    "cnt = -1\n",
    "\n",
    "for handle in handle_names:\n",
    "    cnt +=1\n",
    "    try:\n",
    "        topics = lst[cnt][handle]['LDA']\n",
    "        \n",
    "        if \"machine\" in topics:\n",
    "            print(handle)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# handles to be removed as they do not have valid LDA analysis\n",
    "handle_to_remove = []\n",
    "cnt = -1\n",
    "\n",
    "for handle in handle_names:\n",
    "    cnt += 1\n",
    "    sub_dict = lst[cnt][handle]\n",
    "    \n",
    "    if \"LDA\" not in sub_dict:\n",
    "        handle_to_remove.append(handle)\n",
    "\n",
    "indicies = []\n",
    "\n",
    "for handle in handle_to_remove:\n",
    "    index = handle_names.index(handle)\n",
    "    indicies.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extracting the valid LDA handle\n",
    "verified_handles_lda = [v for i,v in enumerate(handle_names) if i not in frozenset(indicies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meereve', 'VoyageChicago', 'UKDanEdwards', 'SteveWAUGH1979', 'prithwic']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_to_remove[:5] #a peek at the 'bad handles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_object(verified_handles_lda, \"verified_handles_lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_object(handle_to_remove, \"LDA_identified_bad_handles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extracting the appropriate dictionaries to be used in TF-IDF analysis\n",
    "final_database_lda_verified = [v for i,v in enumerate(lst) if i not in frozenset(indicies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_object(final_database_lda_verified, \"final_database_lda_verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
