{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run helper_functions.py\n",
    "%run df_functions.py\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have two databases:\n",
    "\n",
    "1. 2nd degree connection database where all handles have valid LDA Analysis.\n",
    "\n",
    "2. A database with my tweets and associated LDA analysis.\n",
    "\n",
    "The LDA method was quite powerful for potential followers, distilling down their entire corpus to a few key terms.\n",
    "\n",
    "Let's now do some TF-IDF and KMeans clustering to see if we find similar results to LDA.\n",
    "\n",
    "In fact, later in the notebook, I will take the intersection of the LDA Analysis results and TF-IDF results. This intersection will represent words/topics that were picked up by BOTH models for a particular handle's tweets. This will give us the most robust results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gabr_tweets = unpickle_object(\"gabr_ibrahim_tweets_LDA_Complete.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['retweet_count', 'hashtags', 'favorite_count', 'tokenized_tweets', 'content', 'LDA'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gabr_tweets[0]['gabr_ibrahim'].keys() #just to refresh our mind of the keys in the sub-dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now create a TF-IDF model for my tweets.\n",
    "\n",
    "Using K-Means Clustering with TF-IDF, I will cluster my tweet's into 20 centroids. From each of these centroids, I will extract 20 words. These words will be placed in a counter dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF KMeans - segemented by individual tweet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will make use of spacy again in order to ensure we are giving the 'purest' for of our tweets to the `tf-idf` vectorizer.\n",
    "\n",
    "You will see two lists below relating to vocabulary. I will use these lists later to create a usefull dictionary that will help identify particular words within a centroid by index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gabr_df = pd.DataFrame.from_dict(gabr_tweets[0], orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gabr_df = filtration(temp_gabr_df, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gabr_tweets_filtered_1 = dataframe_to_dict(temp_gabr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet_list = []\n",
    "totalvocab_tokenized = []\n",
    "totalvocab_stemmed = []\n",
    "\n",
    "\n",
    "for tweet in gabr_tweets_filtered_1[0]['gabr_ibrahim']['content']:\n",
    "    clean_tweet = \"\"\n",
    "    to_process = nlp(tweet)\n",
    "    \n",
    "    for token in to_process:\n",
    "        if token.is_space:\n",
    "            continue\n",
    "        elif token.is_punct:\n",
    "            continue\n",
    "        elif token.is_stop:\n",
    "            continue\n",
    "        elif token.is_digit:\n",
    "            continue\n",
    "        elif len(token) == 1:\n",
    "            continue\n",
    "        elif len(token) == 2:\n",
    "            continue\n",
    "        else:\n",
    "            clean_tweet += str(token.lemma_) + ' '\n",
    "            totalvocab_tokenized.append(str(token.lemma_))\n",
    "            totalvocab_stemmed.append(str(token.lemma_))\n",
    "            \n",
    "    clean_tweet_list.append(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just going to add this to the dictionary so we can do the second round of filtration\n",
    "gabr_tweets_filtered_1[0]['gabr_ibrahim']['temp_tfidf'] = clean_tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_gabr_df = pd.DataFrame.from_dict(gabr_tweets_filtered_1[0], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_gabr_df = filtration(temp_gabr_df, 'temp_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gabr_tweets_filtered_2 = dataframe_to_dict(temp_gabr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_tweet_list = gabr_tweets_filtered_2[0]['gabr_ibrahim'][\"temp_tfidf\"]\n",
    "del gabr_tweets_filtered_2[0][\"gabr_ibrahim\"][\"temp_tfidf\"] # we will add back TF-IDF analysis later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12065 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('There are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1790, 12785)\n"
     ]
    }
   ],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=200000, stop_words='english', ngram_range=(0,2))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(clean_tweet_list) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 20\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_jobs=-1, random_state=200)\n",
    "\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_dict = dict()\n",
    "for i in range(num_clusters):\n",
    "    for ind in order_centroids[i, :20]: #replace 6 with n words per cluster\n",
    "        word = str(vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0])\n",
    "        if i not in cluster_dict:\n",
    "            cluster_dict[i] = [word]\n",
    "        else:\n",
    "            cluster_dict[i].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_dict.keys() #here we see all 20 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nan',\n",
       " 'turn',\n",
       " 'terrorism',\n",
       " 'identify',\n",
       " 'stable',\n",
       " 'stable',\n",
       " 'secure',\n",
       " 'briton',\n",
       " 'blah',\n",
       " 'battle',\n",
       " 'mi5',\n",
       " 'poll',\n",
       " 'trump',\n",
       " 'swamp',\n",
       " 'turn',\n",
       " 'islamic',\n",
       " 'likely',\n",
       " 'city',\n",
       " 'presently',\n",
       " 'identify']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_dict[0] #words in cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nan',\n",
       " 'report',\n",
       " 'death',\n",
       " 'police',\n",
       " 'follow',\n",
       " 'blast',\n",
       " 'toll',\n",
       " 'death',\n",
       " 'rise',\n",
       " 'shoot',\n",
       " 'turkey',\n",
       " 'explosion',\n",
       " 'ankara',\n",
       " 'war',\n",
       " 'syria',\n",
       " 'attack',\n",
       " 'airport',\n",
       " 'strike',\n",
       " 'turkish',\n",
       " 'syria']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_dict[1] #words in cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nan',\n",
       " 'vote',\n",
       " 'yes',\n",
       " 'scotland',\n",
       " 'turnout',\n",
       " 'yes',\n",
       " 'scotland',\n",
       " 'vote',\n",
       " 'vote',\n",
       " 'remain',\n",
       " 'remain',\n",
       " 'vote',\n",
       " 'result',\n",
       " 'east',\n",
       " 'ayrshire',\n",
       " 'ayrshire',\n",
       " 'scotland',\n",
       " 'total',\n",
       " 'declaration',\n",
       " 'declaration']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_dict[2] #words in cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now lets make our tfidf Counter!\n",
    "cluster_values = []\n",
    "\n",
    "for k, v in cluster_dict.items():\n",
    "    cluster_values.extend(v)\n",
    "\n",
    "counter_gabr_tfidf = Counter(cluster_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'abc': 2,\n",
       "         'accept': 1,\n",
       "         'access': 2,\n",
       "         'accord': 1,\n",
       "         'aid': 1,\n",
       "         'airport': 1,\n",
       "         'allen': 2,\n",
       "         'amazing': 2,\n",
       "         'ambush': 1,\n",
       "         'ankara': 2,\n",
       "         'announce': 1,\n",
       "         'archive': 1,\n",
       "         'area': 1,\n",
       "         'argue': 1,\n",
       "         'ashamed': 2,\n",
       "         'assistance': 2,\n",
       "         'atrocity': 2,\n",
       "         'attack': 4,\n",
       "         'attempt': 1,\n",
       "         'attorney': 1,\n",
       "         'ayrshire': 2,\n",
       "         'battle': 1,\n",
       "         'bbc': 3,\n",
       "         'big': 2,\n",
       "         'blah': 1,\n",
       "         'blast': 1,\n",
       "         'blog': 3,\n",
       "         'boil': 2,\n",
       "         'bomb': 1,\n",
       "         'break': 4,\n",
       "         'brilliant': 2,\n",
       "         'briton': 1,\n",
       "         'broxbourne': 1,\n",
       "         'capital': 1,\n",
       "         'change': 1,\n",
       "         'check': 2,\n",
       "         'chemical': 1,\n",
       "         'child': 1,\n",
       "         'city': 1,\n",
       "         'clinton': 2,\n",
       "         'collaboration': 1,\n",
       "         'complete': 1,\n",
       "         'conda': 1,\n",
       "         'conley': 1,\n",
       "         'control': 1,\n",
       "         'council': 1,\n",
       "         'counterinsurgency': 1,\n",
       "         'country': 1,\n",
       "         'coup': 2,\n",
       "         'crowd': 1,\n",
       "         'cut': 1,\n",
       "         'cyber': 2,\n",
       "         'datum': 4,\n",
       "         'day': 2,\n",
       "         'death': 2,\n",
       "         'declaration': 2,\n",
       "         'deep': 2,\n",
       "         'defend': 2,\n",
       "         'definition': 1,\n",
       "         'die': 1,\n",
       "         'digital': 2,\n",
       "         'donald': 2,\n",
       "         'east': 1,\n",
       "         'egypt': 1,\n",
       "         'erdogan': 1,\n",
       "         'everybody': 2,\n",
       "         'evidence': 1,\n",
       "         'expert': 1,\n",
       "         'explosion': 1,\n",
       "         'expose': 1,\n",
       "         'eye': 3,\n",
       "         'facilitator': 1,\n",
       "         'fact': 2,\n",
       "         'follow': 1,\n",
       "         'force': 1,\n",
       "         'foreign': 1,\n",
       "         'forever': 1,\n",
       "         'forward': 1,\n",
       "         'frog': 1,\n",
       "         'fuck': 1,\n",
       "         'general': 3,\n",
       "         'gift': 1,\n",
       "         'good': 1,\n",
       "         'government': 3,\n",
       "         'great': 1,\n",
       "         'guess': 1,\n",
       "         'hacker': 1,\n",
       "         'head': 1,\n",
       "         'helicopter': 1,\n",
       "         'history': 1,\n",
       "         'house': 1,\n",
       "         'humanitarian': 2,\n",
       "         'identify': 2,\n",
       "         'immigration': 1,\n",
       "         'incredible': 1,\n",
       "         'internet': 2,\n",
       "         'introduction': 2,\n",
       "         'iranian': 2,\n",
       "         'isis': 1,\n",
       "         'islamic': 3,\n",
       "         'issue': 1,\n",
       "         'john': 2,\n",
       "         'kill': 3,\n",
       "         'know': 5,\n",
       "         'late': 1,\n",
       "         'later': 2,\n",
       "         'law': 1,\n",
       "         'lawful': 2,\n",
       "         'learn': 1,\n",
       "         'learning': 1,\n",
       "         'leave': 4,\n",
       "         'lecture': 1,\n",
       "         'lepage': 1,\n",
       "         'let': 1,\n",
       "         'like': 5,\n",
       "         'likely': 1,\n",
       "         'live': 1,\n",
       "         'look': 4,\n",
       "         'love': 2,\n",
       "         'machine': 2,\n",
       "         'mass': 2,\n",
       "         'mcgill': 2,\n",
       "         'mean': 1,\n",
       "         'meet': 1,\n",
       "         'mi5': 1,\n",
       "         'military': 1,\n",
       "         'mom': 2,\n",
       "         'money': 1,\n",
       "         'nan': 20,\n",
       "         'new': 4,\n",
       "         'news': 3,\n",
       "         'nicola': 2,\n",
       "         'nigerian': 1,\n",
       "         'obama': 1,\n",
       "         'office': 1,\n",
       "         'official': 1,\n",
       "         'online': 2,\n",
       "         'open': 2,\n",
       "         'order': 1,\n",
       "         'panel': 2,\n",
       "         'paralysis': 2,\n",
       "         'pay': 1,\n",
       "         'peel': 1,\n",
       "         'people': 4,\n",
       "         'personal': 2,\n",
       "         'police': 2,\n",
       "         'political': 1,\n",
       "         'poll': 2,\n",
       "         'possibly': 2,\n",
       "         'post': 1,\n",
       "         'prepare': 1,\n",
       "         'presently': 1,\n",
       "         'president': 1,\n",
       "         'prevention': 2,\n",
       "         'problem': 1,\n",
       "         'program': 1,\n",
       "         'project': 3,\n",
       "         'putin': 1,\n",
       "         'reaction': 1,\n",
       "         'read': 1,\n",
       "         'record': 2,\n",
       "         'remain': 2,\n",
       "         'renee': 1,\n",
       "         'report': 2,\n",
       "         'result': 3,\n",
       "         'right': 1,\n",
       "         'rise': 1,\n",
       "         'run': 1,\n",
       "         'say': 1,\n",
       "         'science': 1,\n",
       "         'scientist': 1,\n",
       "         'scotland': 4,\n",
       "         'secure': 1,\n",
       "         'security': 3,\n",
       "         'share': 1,\n",
       "         'shit': 1,\n",
       "         'shoot': 1,\n",
       "         'silence': 1,\n",
       "         'simply': 1,\n",
       "         'snp': 2,\n",
       "         'social': 1,\n",
       "         'soldier': 2,\n",
       "         'special': 2,\n",
       "         'stable': 2,\n",
       "         'state': 2,\n",
       "         'strike': 1,\n",
       "         'strong': 1,\n",
       "         'student': 1,\n",
       "         'sturgeon': 1,\n",
       "         'sunderland': 1,\n",
       "         'support': 1,\n",
       "         'surveillance': 2,\n",
       "         'swamp': 1,\n",
       "         'sydney': 1,\n",
       "         'syria': 5,\n",
       "         'taliban': 2,\n",
       "         'talk': 1,\n",
       "         'tax': 2,\n",
       "         'team': 1,\n",
       "         'tell': 4,\n",
       "         'terrorism': 1,\n",
       "         'terrorist': 1,\n",
       "         'thank': 2,\n",
       "         'thing': 1,\n",
       "         'time': 2,\n",
       "         'today': 1,\n",
       "         'toll': 1,\n",
       "         'total': 1,\n",
       "         'tragedy': 2,\n",
       "         'training': 1,\n",
       "         'troop': 1,\n",
       "         'trump': 3,\n",
       "         'try': 1,\n",
       "         'tupac': 1,\n",
       "         'turkey': 2,\n",
       "         'turkish': 3,\n",
       "         'turn': 2,\n",
       "         'turnout': 1,\n",
       "         'ukraine': 1,\n",
       "         'ukrainian': 2,\n",
       "         'united': 2,\n",
       "         'upcoming': 1,\n",
       "         'video': 1,\n",
       "         'view': 2,\n",
       "         'vote': 6,\n",
       "         'want': 2,\n",
       "         'war': 1,\n",
       "         'warn': 1,\n",
       "         'way': 1,\n",
       "         'webcast': 2,\n",
       "         'westminster': 2,\n",
       "         'white': 1,\n",
       "         'work': 3,\n",
       "         'world': 1,\n",
       "         'write': 1,\n",
       "         'yay': 2,\n",
       "         'yes': 2})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_gabr_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gabr_tweets_filtered_2[0]['gabr_ibrahim'][\"tfid_counter\"] = counter_gabr_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gabr_tfidf_counter = gabr_tweets_filtered_2[0]['gabr_ibrahim'][\"tfid_counter\"]\n",
    "\n",
    "gabr_lda_counter = gabr_tweets_filtered_2[0]['gabr_ibrahim'][\"LDA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabr_tfidf_set = set()\n",
    "gabr_lda_set = set()\n",
    "\n",
    "for key, value in gabr_tfidf_counter.items():\n",
    "    gabr_tfidf_set.add(key)\n",
    "\n",
    "for key, value in gabr_lda_counter.items():\n",
    "    gabr_lda_set.add(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "intersection = gabr_tfidf_set.intersection(gabr_lda_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabr_tweets_filtered_2[0]['gabr_ibrahim'][\"lda_tfid_intersection\"] = intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_object(gabr_tweets_filtered_2, \"FINAL_GABR_DATABASE_LDA_TFIDF_VERIFIED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats all there is to this process! I will now write a script called `kmeans.py` that will dyanmically run all the code above for individuals in `final_database_LDA_verified.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
